# üöÄ Data Engineering Full Interview Notes
## ADF + Databricks + Spark (Complete Revision Guide)

---

# 1Ô∏è‚É£ Spark Versions & Databricks Runtime

## Spark Versions
- Spark 1.x
- Spark 2.x
- Spark 3.x (Most used in projects)

## Spark 3.x Features
- Adaptive Query Execution (AQE)
- Improved SQL Engine
- Better performance
- Dynamic partition pruning

## Databricks Runtime
- Built on Spark versions
- Includes optimizations like Photon Engine
- Includes Delta Lake

**Interview One-Liner:**  
Most projects use Spark 3.x on Databricks Runtime for performance and optimization benefits.

---

# 2Ô∏è‚É£ Spark Storage Levels (Caching)

Storage levels define how data is cached.

Types:
- MEMORY_ONLY
- MEMORY_AND_DISK ‚úÖ (Most used)
- DISK_ONLY
- MEMORY_ONLY_SER
- MEMORY_AND_DISK_SER

**Why Used?**
- Avoid recomputation
- Improve performance

**Interview One-Liner:**  
In real-time projects, we mostly use MEMORY_AND_DISK for large datasets.

---

# 3Ô∏è‚É£ Find 3rd Highest Bonus (Current Year)

## PySpark

```python
from pyspark.sql import functions as F
from pyspark.sql.window import Window

current_year = 2026
df_filtered = df.filter(F.col("year") == current_year)

w = Window.orderBy(F.col("bonus").desc())

df_ranked = df_filtered.withColumn("rnk", F.dense_rank().over(w))

result = df_ranked.filter(F.col("rnk") == 3)\
                  .select("emp_name", "bonus")

result.show()
```

## SQL

```sql
SELECT employee_name, bonus
FROM (
    SELECT employee_name,
           bonus,
           DENSE_RANK() OVER (ORDER BY bonus DESC) AS rnk
    FROM employee_bonus
    WHERE year = YEAR(GETDATE())
) t
WHERE rnk = 3;
```

---

# 4Ô∏è‚É£ Word Count

## PySpark

```python
from pyspark.sql.functions import explode, split, col

df = spark.read.text("/path/file.txt")
words = df.select(explode(split(col("value"), " ")).alias("word"))
word_count = words.groupBy("word").count()
word_count.show()
```

## Python

```python
from collections import Counter

data = ["Bangalore","Hyderabad","Mumbai",
        "London","India","UK","Bangalore"]

word_count = Counter(data)

for word, cnt in word_count.items():
    print(word, cnt)
```

---

# 5Ô∏è‚É£ Only 90% Data Loaded in ADF ‚Äì How to Fix

## Common Causes
- Wrong watermark filter
- Inner join dropping rows
- Transformation filters
- Data type errors
- Partial pipeline failure

## Steps to Fix

### 1. Compare Counts
```sql
SELECT COUNT(*) FROM source_table;
SELECT COUNT(*) FROM target_table;
```

### 2. Find Missing Records
```sql
SELECT s.id
FROM source_table s
LEFT JOIN target_table t
ON s.id = t.id
WHERE t.id IS NULL;
```

### 3. Check Copy Activity Logs
- rowsRead
- rowsCopied
- rowsSkipped

### 4. Fix Incremental Logic
Use:
```
WHERE updated_date >= last_success_date
```

### 5. Use MERGE (Upsert)

**Interview One-Liner:**  
I compare source and target counts, identify missing records using left join, fix incremental logic, and reload missing data safely.

---

# 6Ô∏è‚É£ Roles & Responsibilities

- Migration project handling
- Data ingestion into Bronze layer
- Data cleaning in Silver layer
- Business rule implementation
- Gold layer creation (Fact & Dimension)
- SCD implementation
- Performance tuning
- CI/CD deployment
- Monitoring & production support

---

# 7Ô∏è‚É£ Architecture (ADF + Databricks + ADLS)

On-Prem SQL Server  
‚Üì  
Self-Hosted Integration Runtime  
‚Üì  
ADF Copy Activity  
‚Üì  
ADLS Gen2 Bronze  
‚Üì  
Databricks Processing  
‚Üì  
Silver Layer  
‚Üì  
Gold Layer  
‚Üì  
Power BI  

---

# 8Ô∏è‚É£ Parameters in ADF

## Local Parameters (Pipeline Level)
- Used inside one pipeline
- Example:
```
@pipeline().parameters.table_name
```

## Global Parameters (ADF Level)
- Used across pipelines
- Example:
```
@pipeline().globalParameters.env
```

## Benefits
- Reusability
- Dynamic processing
- Less duplication
- Easy maintenance

---

# 9Ô∏è‚É£ Read On-Prem SQL Server in ADF

## Steps
1. Install Self-Hosted Integration Runtime
2. Create SQL Server Linked Service
3. Create Dataset
4. Use Copy Activity

Other Activities:
- Lookup
- Stored Procedure
- Get Metadata
- Data Flow

---

# üîü Metadata in ADF

Metadata = Data about data.

Examples:
- File name
- Size
- Row count
- Last modified date
- Schema

## Activity Used
Get Metadata

## Real-Time Use
- Dynamic file loading
- Incremental loads
- File validation
- Schema validation

---

# 1Ô∏è‚É£1Ô∏è‚É£ Pipeline Failed ‚Äì Safe Rerun Strategy

## Steps
1. Check last successful watermark
2. Reload from last success date
3. Use MERGE to avoid duplicates
4. Validate row counts
5. Update watermark after success

If watermark not available:
```
WHERE date >= current_date - 3
```

---

# 1Ô∏è‚É£2Ô∏è‚É£ Spark vs Databricks Engine

## Spark Engine
- Core processing engine
- Distributed computing
- Handles ETL and transformations

## Databricks Engine
- Optimized Spark platform
- Photon Engine
- Delta Lake
- Easy cluster management

---

# 1Ô∏è‚É£3Ô∏è‚É£ Spark Joins

## Types
- Inner (Default)
- Left
- Right
- Full
- Broadcast

Default Join:
```python
df1.join(df2, "id")  # Inner join
```

---

# 1Ô∏è‚É£4Ô∏è‚É£ Spark Architecture

## Components
- Driver
- Cluster Manager
- Worker Nodes
- Executors
- Tasks

## Flow
User Code  
‚Üì  
Driver (Creates DAG)  
‚Üì  
Cluster Manager  
‚Üì  
Executors  
‚Üì  
Tasks  

## Transformations
- Narrow (No shuffle)
- Wide (Shuffle)

---

# 1Ô∏è‚É£5Ô∏è‚É£ Shuffling in Spark

Shuffling = Data redistribution across partitions.

Occurs in:
- join
- groupBy
- orderBy
- distinct

## Why Costly?
- Network transfer
- Disk I/O
- Memory usage

## Default Shuffle Partitions
```python
spark.sql.shuffle.partitions = 200
```

---

# 1Ô∏è‚É£6Ô∏è‚É£ Databricks Workflows ‚Äì Limitations

- Not ideal for complex orchestration
- Limited cross-system integration
- Basic monitoring
- Limited reusability
- Can be costly

Best for:
- Notebook scheduling
- ETL job execution

---

# 1Ô∏è‚É£7Ô∏è‚É£ Unity Catalog

- Centralized governance layer
- Role-based access control
- Column-level security
- Data lineage
- Audit tracking
- Storage credential management

---

# 1Ô∏è‚É£8Ô∏è‚É£ Handling Bad CSV Data in ADF

## Issues
- Comma inside values
- Schema mismatch
- Column shifts

## Best Practice
CSV ‚Üí Bronze (Raw)  
‚Üì  
Clean in Databricks  
‚Üì  
Silver  

## In Databricks
```python
df = spark.read.option("header",True)\
               .option("inferSchema",True)\
               .csv("path")
```

Use split(), replace(), schema validation.

---

# 1Ô∏è‚É£9Ô∏è‚É£ How to Confirm Data Loaded Successfully

## 1. Compare Row Counts
```sql
SELECT COUNT(*) FROM source;
SELECT COUNT(*) FROM target;
```

## 2. Missing Record Check
```sql
SELECT s.id
FROM source s
LEFT JOIN target t
ON s.id = t.id
WHERE t.id IS NULL;
```

## 3. Audit Table
Store:
- source_count
- target_count
- load_date
- status

## 4. Checksum Validation (Large Data)

---

# üî• Final Rapid Revision Points

- Bronze ‚Üí Silver ‚Üí Gold Architecture
- Use watermark for incremental load
- Use MERGE to prevent duplicates
- MEMORY_AND_DISK for caching
- Default join = Inner
- Default shuffle partitions = 200
- Use Self-Hosted IR for on-prem SQL
- Always validate source vs target counts
- Maintain audit table
- Use metadata-driven pipelines

---

# ‚úÖ END OF FULL INTERVIEW NOTES
